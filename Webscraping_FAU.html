<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>Web Scraping mit R</title>
    <meta charset="utf-8" />
    <meta name="author" content="Fabian Gülzau" />
    <meta name="date" content="2019-09-26" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <link href="libs/str_view/str_view.css" rel="stylesheet" />
    <script src="libs/str_view-binding/str_view.js"></script>
    <script src="libs/kePrint/kePrint.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Web Scraping mit R
### Fabian Gülzau
### Humboldt-Universität zu Berlin
### 2019-09-26

---


# Inhalt



1. Einleitung
2. Web Scraping
    + Grundlagen
    + Web Scraping in R
        + a) Generelles Schema
        + b) Spezielle Technologien
    + Rechtliche &amp; ethische Aspekte
3. Fragen aus den Projekten
4. Ausblick &amp; Weiterlernen

---

# Zur Präsentation

- Folien sind online verfügbar: https://github.com/FabianFox/Webscraping-FAU-Slides
- Beispiele auf [GitHub](https://github.com/FabianFox/Webscraping-FAU-/tree/master/code)
- Benötigte Pakete über: 


```r
install.packages(pacman) # Installation nur einmal notwendig
library(pacman)
p_load(tidyverse, httr, robotstxt, qdap, janitor,
       devtools, lubridate, rzeit2)
```

---

# 1. Einleitung

&gt; If programming is magic, then *web scraping* is wizardry ([Mitchell 2015: vii](https://www.oreilly.com/library/view/web-scraping-with/9781491985564/))

Ziel des Workshops:

- TeilnehmerInnen in die arkane Kunst des Web Scrapings einführen
    - eigene (kleine) Projekte umsetzen
    - Tools kennenlernen
    - Weiterlernen ermöglichen

---

# Sinkende Ausschöpfungsquoten

- Ausschöpfungsquoten repräsentativer Umfragen sinken drastisch ([Farrell/Petersen 2010: 116f.](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1475-682X.2009.00318.x#))
- Ausschöpfungsquote im ALLBUS ([Schwemmer 2019](https://github.com/cschwem2er/allbus_responserates))

&lt;img src="figures/schwemmer2019_allbus.png" width="80%" /&gt;

---

# Internetnutzung

&lt;img src="Webscraping_FAU_files/figure-html/internet_use_fig-1.png" width="60%" height="60%" /&gt;

Quelle: [Eigene Darstellung](https://github.com/FabianFox/Webscraping-FAU-/blob/master/code/internet_use_de_eurostat.R)

---

# Digitale Daten

&gt; "...something big is going on" ([Salganik 2018: 2](https://www.bitbybitbook.com/en/1st-ed/preface/))

- Übergang vom analogen zum digitalen Zeitalter
    - kommende Krise der empirischen Soziologie ([Savage &amp; Burrows 2007](https://journals.sagepub.com/doi/10.1177/0038038507080443#articleShareContainer))?
    - neue Möglichkeiten und/oder Gefahren ([Salganik 2018: 17-41](https://www.bitbybitbook.com/en/1st-ed/observing-behavior/characteristics/))
    
&gt; "'computational social science' (CSS) is occurring. The question is whether
&gt; it happens with or without social scientists" ([Heiberger &amp; Riebling 2016: 1](https://journals.sagepub.com/doi/abs/10.1177/2059799115622763))

---

# Digitale Daten

.pull-left[

- positiv: big, always-on, nonreactive
- negativ: incomplete, inaccessible, nonrepresentative, drifting, algorithmically 
confounded, dirty, sensitive

Quelle: [Salganik 2018: 17-41](https://www.bitbybitbook.com/en/1st-ed/observing-behavior/characteristics/)

]

.pull-right[

&lt;img src="figures/salganik_book.png" width="80%" /&gt;


]

---

# Technologien des WWW

.pull-left[

Infrastruktur des Internets im Alltag unsichtbar.

Unser Browser übernimmt:

- Serveranfragen (Request/Response-Paar: **HTTP**)
- Darstellung von **HTML**, **CSS** und **JavaScript**
- weitere Technologien: AJAX, PHP, SQL... ([Munzert et al. 2015](http://r-datacollection.com/))

Um Informationen gezielt abzufragen, benötigen wir allerdings basale Kenntnisse 
der zugrundeliegenden Technologien.

]

.pull-right[

&lt;img src="figures/sea_cable.jpg" width="80%" /&gt;

]

---

# Beispielprojekt: Studium.org

- Informationen über Studienorte (Einwohnerzahl, Semesterticket (€), Anzahl d. Kinos...) 

&lt;img src="figures/studium_org_example_fau.png" width="70%" /&gt;

Quelle: [studium.org](https://www.studium.org/erziehungswissenschaft/erlangen-n%C3%BCrnberg)

---

# HTTP

**H**yper**t**ext **T**ransfer **P**rotocol [(HTTP)](https://en.wikipedia.org/wiki/Hypertext_Transfer_Protocol)

&lt;img src="figures/request_response_pair.png" width="50%" /&gt;

- Übertragungsprotokoll, welches aus zwei Teilen besteht:

    - Request (Client)
    - Response (Server)

---

# URL

- Requests erfolgen über **U**niform **R**esource **L**ocators [(URL)](https://en.wikipedia.org/wiki/URL)
    
&lt;img src="figures/url.png" width="100%" /&gt;

Quelle: [Evans (2019)](https://jvns.ca/blog/2019/09/12/new-zine-on-http/)

---

# URL

- URL kann in Schleifen genutzt werden, um über Seite zu springen

&lt;img src="figures/loop_url.png" width="85%" /&gt;

---

# Beispiel: HTTP

- `GET`-Abfrage mit `httr::GET` und Antwort des Servers in R


```r
# install.packages("httr")
p_load(httr)
response &lt;- GET("https://www.studium.org/erziehungswissenschaft/suche/?view=uni") %&gt;%
  print()
```

```
## Response [https://www.studium.org/erziehungswissenschaft/suche/?view=uni]
##   Date: 2019-09-26 17:21
##   Status: 200
##   Content-Type: text/html; charset=utf-8
##   Size: 78 kB
## &lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://w...
## &lt;html xmlns="http://www.w3.org/1999/xhtml" xml:lang="de" lang="de"&gt;
## &lt;head&gt;
## &lt;meta http-equiv="content-type" content="text/html; charset=utf-8" /&gt;
## &lt;title&gt;studium.org/&lt;/title&gt;
## &lt;base href="https://www.studium.org/" /&gt;
## &lt;meta name="description" content="Dein Studium, dein Portal. Geschichte,...
## &lt;!--&lt;meta name="description" content="" /&gt;//--&gt;
## &lt;link rel="shortcut icon" href="favicons/erziehungswissenschaft.ico" /&gt;
## &lt;link href="css/MyFontsWebfontsKit.css" rel="stylesheet" type="text/css"...
## ...
```

---

# Beispiel: HTTP

&lt;img src="figures/response_example.png" width="100%" /&gt;

- [Status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)

    - 2XX : Success
    - 3xx : Redirect
    - 4xx : Client-Error
    - 5xx : Server-Error

---

# Beispiel: HTTP


```r
study.df &lt;- response %&gt;% 
  read_html() %&gt;% 
  html_nodes(".fs3.mr_1b") %&gt;%
  html_text() %&gt;%
  tibble(studienort = .)
```

- [Code](https://github.com/FabianFox/Webscraping-FAU-/blob/master/code/pedagogy_programs_example.R) zur Grafik

&lt;img src="Webscraping_FAU_files/figure-html/http_example_fig-1.png" width="45%" /&gt;

---

# HTML

- **H**yper**t**ext **M**arkup **L**anguage [(HTML)](https://www.w3schools.com/html/default.asp)

    - einfacher Text mit zusätzlichen Anweisungen (Tags, Attributes...)
    - wird vom Browser interpretiert und dargestellt
    
- HTML-Code ist über die Webentwicklungstools des Browsers verfügbar
    - Rechtsklick -&gt; Seitenquelltext anzeigen
    - Aufschluss über verfügbare Informationen

&lt;img src="figures/source_code_html.png" width="90%" /&gt;

---

# Beispiel: HTML I

Einfache Seiten über Editor/Notepad++ erstellbar:


```r
&lt;!DOCTYPE html&gt;
&lt;html&gt;

&lt;head&gt;
&lt;title&gt;Workshop: Web Scraping&lt;/title&gt;
&lt;/head&gt;

&lt;body&gt;
&lt;h1&gt; Web Scraping mit R, FAU Erlangen-Nürnberg&lt;/h1&gt;
&lt;p&gt;Dieser Kurs f&amp;uuml;hrt in das "Web Scraping" mit R ein. Er wird durch &lt;a href="https://fguelzau.rbind.io/"&gt;Fabian G&amp;uuml;lzau&lt;/a&gt; geleitet.&lt;/p&gt;

&lt;/body&gt;
&lt;/html&gt;
```

Der Browser interpretiert den Code und zeigt eine [Internetseite](http://htmlpreview.github.io/?https://github.com/FabianFox/Webscraping-FAU-/blob/master/code/HTML-Example.html) an.

---

# Beispiel: HTML II

HTML:

- besitzt eine Baumstruktur
- Markup-Elemente helfen uns Informationen zu lokalisieren (*Relationalität*)
    - **D**ocument **O**bject **M**odel [(DOM)](https://en.wikipedia.org/wiki/Document_Object_Model)
    - Darstellung von HTML in R (*parsing*)
    
&lt;img src="figures/tree_structure.png" width="85%" /&gt;

---

# CSS

**C**ascading **S**tyle **S**heet [(CSS)](https://www.w3schools.com/css/default.asp)

- beinhaltet Informationen über den Stil der HTML-Seite
- kann genutzt werden, um Informationen zu lokalisieren (CSS-Selektoren)

&lt;img src="figures/css_example.png" width="30%" /&gt;

---

# JavaScript

[JavaScript](https://www.w3schools.com/js/default.asp) (JS): "Programmiersprache des Internets"

- macht Internetseiten dynamisch
- Inhalte erscheinen erst nach Ausführung von JS 
    - problematisch für unsere Scraper
    - [Beispiel](https://artfacts.net/):

&lt;img src="figures/js_example.png" width="50%" /&gt;

--- 

# 2a) Web Scraping in R: Generelles Vorgehen

Web Scraping-Projekte folgen einem generellen Schema:

1. Internetseite kennenlernen
2. Import von HTML-Dateien
3. Information isolieren
4. Iteration (Schritt 2&amp;3 wiederholt ausführen)

&lt;img src="figures/data_science_wflow.png" width="50%" /&gt;

Quelle: [Wickham &amp; Grolemund (2018)](https://r4ds.had.co.nz/introduction.html)

---

# Web Scraping in R

&lt;img src="figures/rvest_hex.png" width="8%" /&gt;

.pull-left[

1. Internetseite kennenlernen
2. Import von HTML-Dateien
3. Information isolieren
4. Iteration

]

--

.pull-right[

Das generelle Schema lässt sich in R über das Paket [`rvest`](https://github.com/hadley/rvest) umsetzen:

1. u.a. [`robotstxt`](https://github.com/ropensci/robotstxt)
2. `read_html`: Import von HTML/XML-Dateien
3. Information isolieren
    - `html_nodes`: Extrahiert Teile des HTML-Dokument mit Hilfe von XPath/CSS-Selektoren
    - `html_text` / `html_attr` / `html_table`: Extrahiert Text, Attribute und Tabellen
4. `map` (Paket: [purrr](https://purrr.tidyverse.org/)): Iteration

]

--

---

# Umsetzung in R: Studium.org

- Vorgehen (*decomposition*)
    - am Einzelfall erproben
    - dann über alle Seiten generalisieren

&lt;img src="figures/decomposition_bbc.png" width="65%" /&gt;

Quelle: [BBC (2019)](https://www.bbc.co.uk/bitesize/topics/zkcqn39/articles/z8ngr82)

---

# 1. Internetseite kennenlernen

Wichtige Punkte:

- statische oder dynamische Internetseite
    - statisch: [`rvest`](https://github.com/hadley/rvest)
    - dynamisch
        - [`RSelenium`](https://github.com/ropensci/RSelenium)
        - [`splashr`](https://github.com/hrbrmstr/splashr)
- Rechtliche Punkte
    - Terms of Service (ToS)
    - robots.txt
    - API
- Verhalten der URL beobachten

---

# Studium.org: Internetseite kennenlernen

Einzelne Punkte:

- Seitenquelltext: **statisch**
- HTML-Tags der Zielinformation: **.right**
- robots.txt / Terms of Service: **keine relevanten Beschränkungen**


```r
paths_allowed(
  paths  = c("/erziehungswissenschaft"), 
  domain = c("studium.org")
)
```

```
## 
 studium.org                      No encoding supplied: defaulting to UTF-8.
```

```
## [1] TRUE
```

---

# 2. Import von HTML-Dateien

Internetseiten müssen in ein Format übersetzt werden, welches von R gelesen und
bearbeitet werden kann (z.B. Baumstruktur).

Benötigt wird:

- URL 
- Request/Response-Paar

---

# Studium.org: Import von HTML-Seiten

Über `rvest::read_html`:


```r
(html.page &lt;- read_html("https://www.studium.org/erziehungswissenschaft/erlangen-n%C3%BCrnberg"))
```

```
## {xml_document}
## &lt;html xmlns="http://www.w3.org/1999/xhtml" xml:lang="de" lang="de"&gt;
## [1] &lt;head&gt;\n&lt;meta http-equiv="content-type" content="text/html; charset= ...
## [2] &lt;body class="ff1 fs3 bg4"&gt;\r\n&lt;div class="bg3 block center mainHeade ...
```

---

# 3. Information isolieren

Zur Extraktion von Informationen nutzen wir die Baumstruktur der HTML:

- [XPath](https://www.w3schools.com/xml/xpath_intro.asp)
- [CSS-Selektoren](https://www.w3schools.com/csSref/css_selectors.asp)

&lt;img src="figures/trees_structure_2.png" width="80%" height="85%" /&gt;

Quelle: [selfhtml.de](https://wiki.selfhtml.org/wiki/XML/Regeln/Baumstruktur)

---

# XPath und CSS-Selektoren: Tools

Wir konstruieren Selektoren selten manuell, da Anwendungen dies übernehmen:

- [Selector Gadget](https://selectorgadget.com/)
- Browser: [Webentwicklungstools](https://developer.mozilla.org/de/docs/Tools/Seiten_Inspektor)
- Lerntools: [CSS Diner](https://flukeout.github.io/)

---

# Studium.org: CSS-Selektor

Mit SelectorGadget lässt sich die gewünschte Information schnell "erklicken".


```r
html.nodes &lt;- html_nodes(html.page, css = ".right")
```


```
## {xml_nodeset (4)}
## [1] &lt;div class="unten right fs2"&gt;\r\n\t\t\t\t\t107.500/513.500 Einwohner ...
## [2] &lt;div class="unten right fs2"&gt;\r\n\t\t\t\t\t39.628 Studierende\r\n\t\ ...
## [3] &lt;div class="unten right fs2"&gt;\r\n\t\t\t\t\t84 Stunden / Woche\r\n\t\ ...
## [4] &lt;div class="unten right fs2"&gt;\r\n\t\t\t\t\t199,00 Euro\r\n\t\t\t\t&lt;/ ...
```

&lt;img src="figures/selector_gadget_example.png" width="80%" height="40%" /&gt;

---

# Studium.org: Umwandeln in Text/Tabelle

Wir sind selten am HTML-Tag interessiert, sondern an dem Inhalt:


```r
html.text &lt;- html.nodes %&gt;%
  html_text()
```


```
## [1] "\r\n\t\t\t\t\t107.500/513.500 Einwohner/innen\r\n\t\t\t\t"
## [2] "\r\n\t\t\t\t\t39.628 Studierende\r\n\t\t\t\t"             
## [3] "\r\n\t\t\t\t\t84 Stunden / Woche\r\n\t\t\t\t"             
## [4] "\r\n\t\t\t\t\t199,00 Euro\r\n\t\t\t\t"
```

---

# Datenaufbereitung: RegEx

.left-column[

&lt;img src="figures/regex_orly.jpg" width="95%" height="95%" /&gt;

]

.right-column[

**Reg**ular **Ex**pression

- zur Suche von Ausdrücken (*patterns*) in Text (*strings*)
- in R z.B. über [`stringr`](https://stringr.tidyverse.org/index.html)
    - viele hilfreiche [Funktionen](https://rstudio.com/resources/cheatsheets/#work-with-strings-cheat-sheet) (u.a. `str_trim`, `str_replace`, `str_extract`)


```r
str_view(string = "Wichtig ist die Zahl 42!",
         pattern = "[:digit:]+")
```

<div id="htmlwidget-ec1cd6a93bd61bfefecc" style="width:960px;height:100%;" class="str_view html-widget"></div>
<script type="application/json" data-for="htmlwidget-ec1cd6a93bd61bfefecc">{"x":{"html":"<ul>\n  <li>Wichtig ist die Zahl <span class='match'>42<\/span>!<\/li>\n<\/ul>"},"evals":[],"jsHooks":[]}</script>

]

---

# RegEx: Studium.org

Die Umfrageergebnisse liegen als "strings" vor, sodass wir sie für die Datenanalyse 
in numerische Werte umwandeln müssen.


```r
html.text &lt;- html.text %&gt;%
  str_replace_all(., "(\\r|\\t|\\n)", "") %&gt;%
  str_extract(., "[:digit:]+,|.[:digit:]+") %&gt;%
  str_replace_all(., ",", ".") %&gt;%
  str_trim(., side = "both") %&gt;%
  enframe()
```


```
## # A tibble: 9 x 2
##    name value
##   &lt;int&gt; &lt;chr&gt;
## 1     1 107  
## 2     2 39   
## 3     3 84   
## 4     4 199. 
## 5     5 8.   
## 6     6 &lt;NA&gt; 
## 7     7 .700 
## 8     8 &lt;NA&gt; 
## 9     9 449
```

---

# Vom Einzelfall zum Gesamtprojekt

Was benötigen wir, um Datensatz zu erstellen, der die Informationen für alle Universitäten enthält?

1. URLs zu den Einzelseiten (`html_attr("href")`)
2. Funktion, die die Einzelschritte generalisiert
3. Iteration über alle Seiten (`map`)
4. Datenaufbereitung ([`stringr`](https://stringr.tidyverse.org/), [`dplyr`](https://dplyr.tidyverse.org/))

&lt;img src="figures/purrr_fig.png" width="80%" /&gt;

Quelle: [purrr-Cheatsheet](https://www.rstudio.com/resources/cheatsheets/#purrr)

---

# 1. URLs zu den Einzelseiten


```r
# (A) Liste mit Links zu den Einzelseiten
url &lt;- "https://www.studium.org/erziehungswissenschaft/uebersicht-universitaeten"

# (B) Node-Attribut mit Link ("href")
links &lt;- url %&gt;%
  read_html() %&gt;%
  html_nodes(".lh1 a") %&gt;%
  html_attr("href") %&gt;%
  tibble(paste0("http://www.studium.org/", .)) %&gt;%
  set_names(., nm = c("uni", "link"))
```


```
## # A tibble: 4 x 2
##   uni                         link                                         
##   &lt;chr&gt;                       &lt;chr&gt;                                        
## 1 erziehungswissenschaft/aug~ http://www.studium.org/erziehungswissenschaf~
## 2 erziehungswissenschaft/bas~ http://www.studium.org/erziehungswissenschaf~
## 3 erziehungswissenschaft/fu-~ http://www.studium.org/erziehungswissenschaf~
## 4 erziehungswissenschaft/hu-~ http://www.studium.org/erziehungswissenschaf~
```

---

# 2. Funktion erstellen

.pull-left[


```r
# Some data
test &lt;- tibble(
  var1 = c(1, 3, 5, 9),
  var2 = c(5, 5, 5, 5)
)

# Function
perc_fun &lt;- function(x){  # formals
  x / sum(x) * 100        # body
}

# Apply
map(test, perc_fun) # Loop
```

```
## $var1
## [1]  5.555556 16.666667 27.777778 50.000000
## 
## $var2
## [1] 25 25 25 25
```

]

.pull-right[

&lt;img src="figures/function_fig.png" width="75%" /&gt;

Vertiefend: [Wickham 2019](https://adv-r.hadley.nz/functions.html)

]

---

# 2. Funktion erstellen (studium.org)


```r
steckbrief_fun &lt;- function(x) {
  read_html(x) %&gt;%           # (2) Import      # Body besteht
    html_nodes(".right") %&gt;% # (3) Extrahieren # aus Einzelfall-
    html_text()              # (4) als Text    # Vorgehen
}

# Save scraping
save_steckbrief_fun &lt;- possibly(steckbrief_fun, NA_real_)
```

Die Funktion `possibly` lässt die Schleife auch dann weiterlaufen, wenn ein Link nicht funktioniert ([s. Vignette](https://purrr.tidyverse.org/reference/safely.html)). 

---

# 3. Iteration über alle Seiten


```r
steckbrief.info &lt;- map(.x = links$link, ~ {
  Sys.sleep(sample(seq(2, 5, by = .5), 1)) # friendly scraping
  save_steckbrief_fun(.x)
})
```


```
## [[1]]
## [1] "\r\n\t\t\t\t\t 272.000 Einwohner/innen\r\n\t\t\t\t"
## 
## [[2]]
## [1] "\r\n\t\t\t\t\t165.041 Einwohner/innen\r\n\t\t\t\t"
## 
## [[3]]
## [1] "\r\n\t\t\t\t\t3.415.000 Einwohner/innen\r\n\t\t\t\t"
## 
## [[4]]
## [1] "\r\n\t\t\t\t\t3.415.000 Einwohner/innen\r\n\t\t\t\t"
```

---

# 4. Datenaufbereitung

Daten sind als `list` in einem `tibble` (dataframe) gespeichert und nicht bereinigt.

Vorgehen:
- (A) Liste benennen
- (B) Liste in "long"-Format überführen 
- (C) Daten bereinigen (RegEx)
- (D) Variablennamen ergänzen

---

# Datenaufbereitung (A &amp; B)

Vorgehen:
- **(A) Liste benennen**
- **(B) Liste in "long"-Format überführen**
- (C) Daten bereinigen (RegEx)
- (D) Variablennamen ergänzen


```r
# (A) Liste benennen
names(steckbrief.info) &lt;- str_extract_all(links$uni, "(?&lt;=/)[:alpha:].*")

# (B) "long"-Format
uni.df &lt;- steckbrief.info %&gt;%
  enframe() %&gt;%
  unnest()
```


```
## # A tibble: 4 x 2
##   name     value                                               
##   &lt;chr&gt;    &lt;chr&gt;                                               
## 1 augsburg "\r\n\t\t\t\t\t 272.000 Einwohner/innen\r\n\t\t\t\t"
## 2 augsburg "\r\n\t\t\t\t\t20.386 Studierende\r\n\t\t\t\t"      
## 3 augsburg "\r\n\t\t\t\t\t98 Stunden / Woche\r\n\t\t\t\t"      
## 4 augsburg "\r\n\t\t\t\t\t60,05 Euro\r\n\t\t\t\t"
```

---

# Datenaufbereitung (C)

Vorgehen:
- (A) Liste benennen
- (B) Liste in "long"-Format überführen
- **(C) Daten bereinigen (RegEx)**
- (D) Variablennamen ergänzen


```r
# (C) Daten bereinigen
uni.df &lt;- uni.df %&gt;%
  mutate(value = 
           str_extract_all(value, "[:digit:]+,|.[:digit:]+") %&gt;%
           map(., paste0, collapse = "") %&gt;%
           unlist() %&gt;%
           str_replace_all("\\.", "") %&gt;%
           str_replace_all(., ",", "\\.") %&gt;%
           parse_number()
  )
```


```
## # A tibble: 4 x 2
##   name        value
##   &lt;chr&gt;       &lt;dbl&gt;
## 1 augsburg 272000  
## 2 augsburg  20386  
## 3 augsburg     98  
## 4 augsburg     60.0
```

---

# Datenaufbereitung (D)

Vorgehen:
- (A) Liste benennen
- (B) Liste in "long"-Format überführen
- (C) Daten bereinigen (RegEx)
- **(D) Variablennamen ergänzen**


```r
# (D) Variablennamen ergänzen: Scrapen
var_names &lt;- read_html(links$link[1]) %&gt;%
  html_nodes(".rel.fs4") %&gt;%
  html_text() %&gt;%
  bracketX() %&gt;%         # Cleaning
  tolower() %&gt;%          # ...
  str_replace_all(.,     # ...
                  " ", 
                  replacement = "_") 

# ...: Ergänzen
uni.df &lt;- uni.df %&gt;%
  mutate(
    var_names = rep(var_names, length(unique(name)))
  ) 
```


```
## # A tibble: 4 x 3
##   name        value var_names                    
##   &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;                        
## 1 augsburg 272000   einwohnerzahl_stadt          
## 2 augsburg  20386   studierende_hochschule_gesamt
## 3 augsburg     98   öffnungszeiten_bibliothek    
## 4 augsburg     60.0 kosten_semesterticket_in_€
```

---

# Ergebnis

&lt;table class="table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; name &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; value &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; var_names &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; augsburg &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 272000.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; einwohnerzahl_stadt &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; augsburg &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 20386.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; studierende_hochschule_gesamt &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; augsburg &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 98.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; öffnungszeiten_bibliothek &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; augsburg &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 60.05 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; kosten_semesterticket_in_€ &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; augsburg &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.65 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; mietspiegel_stadt &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; augsburg &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; NA &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; regionaler_preisindex &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; augsburg &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1720.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; sonnenstunden_pro_jahr &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; augsburg &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 5.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; anzahl_kinos &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; augsburg &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 582.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; km_zum_nächsten_dgfe-kongress &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; basel &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 165041.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; einwohnerzahl_stadt &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; basel &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 11828.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; studierende_hochschule_gesamt &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; basel &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 77.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; öffnungszeiten_bibliothek &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; basel &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; NA &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; kosten_semesterticket_in_€ &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; basel &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; NA &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; mietspiegel_stadt &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; basel &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; NA &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; regionaler_preisindex &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

# Ergebnis: Visualisierung

- [Code](https://github.com/FabianFox/Webscraping-FAU-/blob/master/code/studium_org_erzw_scraper.R) zur Grafik

&lt;img src="Webscraping_FAU_files/figure-html/studium_viz-1.png" width="65%" height="65%" /&gt;

---

# 2b. Spezielle Technologien

Spezielle Möglichkeiten &amp; Herausforderungen:

[**A**pplication **P**rogramming **I**nterface: (API)](https://en.wikipedia.org/wiki/Application_programming_interface)

- **erleichtert** den Zugriff auf Daten über spezifische Schnittstellen
    - Beispiele: [Twitter](https://developer.twitter.com/en.html), [Die Zeit](http://developer.zeit.de/index/), [Eurostat](https://ec.europa.eu/eurostat/web/json-and-unicode-web-services), [Wikipedia](https://github.com/Ironholds/WikipediR/)
    - in R: [rtweet](https://github.com/mkearney/rtweet), [rzeit2](https://github.com/jandix/rzeit2#rzeit2-), [eurostat](https://github.com/rOpenGov/eurostat), [WikipediR](https://github.com/Ironholds/WikipediR/)
    
Dynamische Webseiten &amp; bot blocking:

- **erschwert** Zugriff, da Daten erst nach Nutzeranfragen erzeugt werden
    - Beispiele: Süddeutsche Zeitung (Archiv), ArtFacts, GIZ
    - viele soziale Medien

---

# APIs: Steigende Relevanz

- [Code](https://github.com/FabianFox/Webscraping-FAU-/blob/master/code/programmableweb-Scraper.R) zur Grafik

&lt;img src="Webscraping_FAU_files/figure-html/api_plot-1.png" width="70%" height="70%" /&gt;

---

# Exkurs: APIcalypse

- Skandal um Cambridge Analytica führt zu API-Beschränkungen (*APIcalypse*) ([Bruns 2019](https://www.tandfonline.com/doi/full/10.1080/1369118X.2019.1637447))

Auswege (vgl. Bruns 2019):
- walk away (Wikipedia, Reddit)
- lobby for change 
- accommodate and acquiesce 
- break the rules (web scraping trotz ToS)

---

# API-Beispiel: Die Zeit

APIs erlauben die authentifizierte Abfrage von Informationen. Benötigt wird:

- API-Key (einmalig beantragen)
- R-Schnittstelle zur API (oder `httr`, `plumber`)

&lt;img src="figures/rzeit2.png" width="20%" /&gt;

---

# Kulturelle Bildung in der ZEIT


```r
query &lt;- get_content(query = "\"kulturelle Bildung\"", limit = 250,
                     api_key = api_key) # authentifizieren

map(query$content, 1) %&gt;%
  names()
```


```
##  [1] "uuid"         "title"        "release_date" "uri"         
##  [5] "snippet"      "href"         "teaser_text"  "teaser_title"
##  [9] "subtitle"     "supertitle"
```

---

# rzeit2: Informationen verwenden


```r
# Distribution of articles over time
zeit.df &lt;- tibble(
  date = query$content$release_date) %&gt;%
  mutate(year = strtoi(str_extract(date, "[:digit:]{4}")),
         groups = cut(year, seq(1965, 2020, 5)))
```

&lt;img src="Webscraping_FAU_files/figure-html/rzeit2_fig-1.png" width="50%" height="50%" /&gt;

---

# Rechtliche Aspekte

&gt; "Big data? Cheap. Lawyers? Not so much." (Pete Warden zit. in [Mitchell 2015: 217](http://shop.oreilly.com/product/0636920034391.do))

- rechtliche Grauzone
- [länderspezifisch](https://en.wikipedia.org/wiki/Web_scraping#Legal_issues) (USA: strikt; EU: liberal)
- Terms of Service (ToS) beachten
- robots.txt prüfen

---

# Rechtliche Aspekte: Praxis

**Ziel**: "friendly scraping" 

- Server nicht überlasten ([crawl delay](https://rud.is/b/2017/07/28/analyzing-wait-delay-settings-in-common-crawl-robots-txt-data-with-r/))
    - `Sys.sleep` einbauen (~5-10 Sekunden)
    - zufällige Wartezeit, um menschliches Verhalten zu imitieren
- Bot oder Mensch
    - "headless browsing"
        - [RSelenium](https://github.com/ropensci/RSelenium), [decapitated](https://github.com/hrbrmstr/decapitated), [splashr](https://github.com/hrbrmstr/splashr) oder [htmlunit](https://github.com/hrbrmstr/htmlunit)
- API nutzen (Sammlung unter [programmableweb.com](https://www.programmableweb.com/))
- Seitenbetreiber kontaktieren

---

# 4. Weitere Beispiele und Anwendungen

- JavaScript-Seiten: [ArtFacts](https://github.com/FabianFox/Webscraping-FAU-/blob/master/code/ArtFacts-Scraper.R)
- weitere Beispiele... (Transfermarkt, GIZ, UN-Resolutionen)
- Arbeiten mit PDF-Dokumenten: [`tabulizer`](https://github.com/FabianFox/Webscraping-FAU-/blob/master/code/reading_pdf_example.R) &amp; 'pdftools'
- Diskussion eigener Projekte

---

# 5. Weiterlernen

Bücher:

- Wickham &amp; Grolemund (2017) R for Data Science [(online)](https://r4ds.had.co.nz/)
- Healy (2018) Data Visualization [(online)](https://socviz.co/index.html)
- Phillips (2018) YaRrr! The Pirate’s Guide to R [(online)](https://bookdown.org/ndphillips/YaRrr/)

Interaktiv:

- RStudioPrimers [(online)](https://rstudio.cloud/learn/primers)
- swirl: Learn R, in R [(Installation)](https://swirlstats.com/students.html)

Kurzanleitungen:

- Cheatsheets [(online)](https://www.rstudio.com/resources/cheatsheets/)

Weiterführend:
- Sammlung von Lernressourcen auf RStudio [(online)](https://www.rstudio.com/resources/)
- Lerncommunity: [TidyTuesdays](https://github.com/rfordatascience/tidytuesday)
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
